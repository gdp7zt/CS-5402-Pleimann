{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7e5714d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a1914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62d628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_book(document):\n",
    "    lines = document.split(\"\\n\")\n",
    "    start= 0\n",
    "    end = len(lines)\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        if line.startswith(\"*** START OF THIS PROJECT GUTENBERG\"):\n",
    "            start = i + 1\n",
    "        elif line.startswith(\"*** END OF THIS PROJECT GUTENBERG\"):\n",
    "            end = i - 1\n",
    "    return \"\\n\".join(lines[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b9892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_folder = r'C:\\Users\\gdple\\AppData\\Roaming\\nltk_data\\corpora\\gutenberg'\n",
    "def load_books_data(folder=data_folder):\n",
    "    documents = []\n",
    "    authors = []\n",
    "    subfolders = [subfolder for subfolder in os.listdir(folder)\n",
    "                  if os.path.isdir(os.path.join(folder, subfolder))]\n",
    "    for author_number, subfolder in enumerate(subfolders):\n",
    "        full_subfolder_path = os.path.join(folder, subfolder)\n",
    "        for document_name in os.listdir(full_subfolder_path):\n",
    "            with open(os.path.join(full_subfolder_path, document_name), encoding ='ISO-8859-1') as inf:\n",
    "                documents.append(clean_book(inf.read()))\n",
    "                authors.append(author_number)\n",
    "    return documents, np.array(authors, dtype='int')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c502cbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa2b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents, classes = load_books_data(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f4e95f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gdple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gdple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import glob\n",
    "import os\n",
    "import ntpath\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce30cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = r\"C:\\Users\\gdple\\AppData\\Roaming\\nltk_data\\corpora\\gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07a1ddd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Percy = sorted(glob.glob(os.path.join(papers, \"Percy/*\")))\n",
    "Mary = sorted(glob.glob(os.path.join(papers, \"Mary/*\")))\n",
    "Frankenstein = sorted(glob.glob(os.path.join(papers, \"Disputed Paper/*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55edcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Percy_papers = []\n",
    "for fn in Percy:\n",
    "    with open(fn,encoding ='ISO-8859-1') as f:\n",
    "        Percy_papers.append(f.read().replace('\\n', ' ').replace('\\r',''))\n",
    "Percy_papers_all = ' '.join(Percy_papers)\n",
    "\n",
    "Mary_papers = []\n",
    "for fn in Mary:\n",
    "    with open(fn,encoding ='ISO-8859-1') as f:\n",
    "        Mary_papers.append(f.read().replace('\\n', ' ').replace('\\r',''))\n",
    "Mary_papers_all = ' '.join(Mary_papers)\n",
    "\n",
    "disputed_papers = []\n",
    "disputed_papers_file_names = []\n",
    "for fn in Frankenstein:\n",
    "    with open(fn,encoding ='ISO-8859-1') as f:\n",
    "        disputed_papers.append(f.read().replace('\\n', ' ').replace('\\r',''))\n",
    "        disputed_papers_file_names.append(ntpath.basename(fn))\n",
    "disputed_papers_all = ' '.join(disputed_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f126da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_papers_all = Percy_papers_all + \" \" + Mary_papers_all\n",
    "known_papers = Percy_papers + Mary_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4847203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalFeatures(papers, all_papers):\n",
    "    \"\"\"\n",
    "    Compute feature vectors for word and punctuation features\n",
    "    \"\"\"\n",
    "    num_papers = len(papers)\n",
    "    fvs_lexical = np.zeros((len(papers), 2), np.float64)\n",
    "    fvs_punct = np.zeros((len(papers), 3), np.float64)\n",
    "    for e, single_paper_text in enumerate(papers):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(single_paper_text.lower())\n",
    "        words = word_tokenizer.tokenize(single_paper_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(single_paper_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 1] = len(vocab) / float(len(words))\n",
    "\n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(';') / float(len(sentences))\n",
    "        fvs_punct[e, 1] = tokens.count('\"') / float(len(sentences))\n",
    "        fvs_punct[e, 2] = tokens.count(',') / float(len(sentences))\n",
    "        \n",
    "\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "\n",
    "    return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c65c6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures(papers, all_papers):\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    def token_to_pos(paper):\n",
    "        tokens = nltk.word_tokenize(paper)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "    paper_pos = [token_to_pos(paper) for paper in papers]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[paper.count(pos) for pos in pos_list] for paper in paper_pos]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens in the paper\n",
    "    fvs_syntax /= np.c_[np.array([len(paper) for paper in paper_pos])]\n",
    "\n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1db03f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=100, max_iter=300, verbose=0)\n",
    "    km.fit(fvs)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dd7a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdple\\anaconda3\\lib\\site-packages\\scipy\\cluster\\vq.py:136: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  warnings.warn(\"Some columns have standard deviation zero. \"\n"
     ]
    }
   ],
   "source": [
    "known_set = list(LexicalFeatures(known_papers, known_papers_all))\n",
    "known_set.append(SyntacticFeatures(known_papers, known_papers_all))\n",
    "\n",
    "classifications = [PredictAuthors(fvs) for fvs in known_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "625e81e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdple\\anaconda3\\lib\\site-packages\\scipy\\cluster\\vq.py:136: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  warnings.warn(\"Some columns have standard deviation zero. \"\n"
     ]
    }
   ],
   "source": [
    "disputed_set = list(LexicalFeatures(disputed_papers, disputed_papers_all))\n",
    "disputed_set.append(SyntacticFeatures(disputed_papers, disputed_papers_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef2bd66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "results.append([classifications[0].predict(disputed_set[0]),\"Lexical Features\"]) # Predict results of Lexical Features\n",
    "results.append([classifications[1].predict(disputed_set[1]),\"Lexical Features - Punctuation\"]) # Predict results of Lexical Features, Punctuation\n",
    "results.append([classifications[2].predict(disputed_set[2]),\"Syntactic Features\"]) # Predict results of their syntactic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f6c164b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Percy']\n",
      "['Mary']\n",
      "['Percy']\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for i in range(len(classifications)):\n",
    "    Mary = classifications[i].labels_[0] # Extract first\n",
    "    individual_classifier_results = []\n",
    "    for j in range(len(results[i][0])):\n",
    "        if results[i][0][j] == Mary:\n",
    "            individual_classifier_results.append(\"Mary\")\n",
    "        else:\n",
    "            individual_classifier_results.append(\"Percy\")\n",
    "    print(individual_classifier_results)\n",
    "    all_results.append(individual_classifier_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56fd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ee960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fc3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc0b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acaf84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9fc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c9401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5218d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2befcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
